from sklearn.model_selection import KFold
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.ensemble import AdaBoostRegressor
from sklearn.metrics import mean_squared_error

import os
import math
import numpy as np
import itertools


def load_data(filename):
    """
    Load processed data generated by data_processing script
    :param filename: File to be opened/loaded
    :return: Data as numpy array
    """
    return np.load(filename)


def build_tree(depth, split):
    """
    Builds a Decision Tree Regressor using default values
    Random_seed used for reproducibility
    :return: Tree
    """
    return DecisionTreeRegressor(min_samples_split=split, max_depth=depth, random_state=15)


def build_ensemble(n_est, lr):
    return AdaBoostRegressor(DecisionTreeRegressor(min_samples_split=20, max_depth=20, random_state=15),
                             learning_rate=lr, n_estimators=n_est, random_state=15)


def build_support_machine(c, epsilon,tol):
    return SVR(C=c, epsilon=epsilon, tol=tol)


def tree_experiments(data_array, tree_depth, tree_split, other_coins=False):
    """
    Train Decision Tree using 10-fold cross validation
    :param data_array: Data for one coin
    :return: None
    """

    # Create train/test splits from coin data
    k_splits = 10
    kf = KFold(n_splits=k_splits)

    # Separate data into inputs and outputs.
    # Inputs are 0-98, predictions are 1-99
    inputs = data_array[0:-1]
    # inputs = inputs.reshape(-1,1)
    outputs = data_array[1:, 1]
    # print(outputs)

    # Train and evaluate tree with 10-fold c.v.
    avg_mse = 0
    avg_r = 0
    for train, test in kf.split(inputs):
        tree = build_tree(tree_depth, tree_split)
        x_train, x_test = inputs[train], inputs[test]
        y_train, y_test = outputs[train], outputs[test]
        tree.fit(x_train, y_train)
        pred_y = tree.predict(x_test)
        mse = math.sqrt(mean_squared_error(y_test, pred_y))
        r = tree.score(x_test, y_test)
        avg_mse += mse
        avg_r += r
    avg_mse /= k_splits
    avg_r /= k_splits
    print("\tAverage rmse: ", avg_mse)
    print("\tAverage r2: ", avg_r)


def ensemble_experiments(data_array, n_estimator, learning_rate):
    """
    Train Ada Boost Regressor using 10-fold cross validation
    :param data_array: Data for one coin
    :return: None
    """

    # Create train/test splits from coin data
    k_splits = 10
    kf = KFold(n_splits=k_splits)

    # Separate data into inputs and outputs.
    # Inputs are 0-98, predictions are 1-99
    inputs = data_array[0:-1]
    # inputs = inputs.reshape(-1,1)
    outputs = data_array[1:, 1]
    # print(outputs)

    # Train and evaluate tree with 10-fold c.v.
    avg_mse = 0
    avg_r = 0
    for train, test in kf.split(inputs):
        tree = build_ensemble(n_estimator, learning_rate)
        x_train, x_test = inputs[train], inputs[test]
        y_train, y_test = outputs[train], outputs[test]
        tree.fit(x_train, y_train)
        pred_y = tree.predict(x_test)
        mse = math.sqrt(mean_squared_error(y_test, pred_y))
        r = tree.score(x_test, y_test)
        avg_mse += mse
        avg_r += r
    avg_mse /= k_splits
    avg_r /= k_splits
    print("\tAverage rmse: ", avg_mse)
    print("\tAverage r2: ", avg_r)


def svr_experiments(data_array, c, epsilon, tol):
    """
    Train Decision Tree using 10-fold cross validation
    :param data_array: Data for one coin
    :return: None
    """

    # Create train/test splits from coin data
    k_splits = 10
    kf = KFold(n_splits=k_splits)

    # Separate data into inputs and outputs.
    # Inputs are 0-98, predictions are 1-99
    inputs = data_array[0:-1]
    outputs = data_array[1:,1] # Extract only column 1

    # Train and evaluate tree with 10-fold c.v.
    avg_mse = 0
    avg_r = 0
    for train, test in kf.split(inputs):
        machine = build_support_machine(c, epsilon, tol)
        x_train, x_test = inputs[train], inputs[test]
        y_train, y_test = outputs[train], outputs[test]
        machine.fit(x_train, y_train)
        pred_y = machine.predict(x_test)
        mse = math.sqrt(mean_squared_error(y_test, pred_y))
        r = machine.score(x_test, y_test)
        avg_mse += mse
        avg_r += r
    avg_mse /= k_splits
    avg_r /= k_splits
    print("\tAverage rmse: ", avg_mse)
    print("\tAverage r2: ", avg_r)


if __name__ in "__main__":
    direct = "./np_coin_data/"

    # Decision Tree Experiments
    #   Predicting for own coin
    # max_depth = [1,2,3,4,5,6,7,8,9,10,15,20,25,50]
    # min_split = [2,3,4,5,6,7,8,9,10,15,20,25,50]
    # print("Beginning TREE Grid Search:")
    # for param in itertools.product(max_depth,min_split):
    #     print(param)
    #     for i in os.listdir(direct):
    #         print("\tTraining on ", i.split(".")[0])
    #         bit_data = load_data(os.path.join(direct, i))
    #         tree_experiments(bit_data, param[0], param[1])

    # AdaBoost Regressor Experiments
    #   Predicting on own coin
    estimators = [50, 75, 100, 200]
    learn_rate = [1.0, 0.5, 0.1, 0.01]
    print("Beginning ADA Grid Search:")
    for param in itertools.product(estimators, learn_rate):
        print(param)
        for i in os.listdir(direct):
            print("\tTraining on ", i.split(".")[0])
            bit_data = load_data(os.path.join(direct, i))
            ensemble_experiments(bit_data, param[0], param[1])

    # Decision Tree Experiments
    #   Predicting for other coins
    # max_depth = [1,2,3,4,5,6,7,8,9,10,15,20,25,50]
    # min_split = [2,3,4,5,6,7,8,9,10,15,20,25,50]
    # print("Beginning TREE Grid Search:")
    # for param in itertools.product(max_depth,min_split):
    #     print(param)
    #     for i in os.listdir(direct):
    #         print("\tTraining on ", i.split(".")[0])
    #         bit_data = load_data(os.path.join(direct, i))
    #         tree_experiments(bit_data, param[0], param[1], True)

    # c_vals = [0.1, 0.5, 0.75, 1.0, 1.5]
    # epsilons = [0.01, 0.05, 0.1, 0.5, 1.0]
    # tols = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1]
    # print("Beginning SVR Grid Search: ")
    # for param in itertools.product(c_vals,epsilons, tols):
    #     print(param)
    #     for i in os.listdir(direct):
    #         print("\t Training on ", i.split(".")[0])
    #         bit_data = load_data(os.path.join(direct, i))
    #         svr_experiments(bit_data,param[0],param[1],param[2])

